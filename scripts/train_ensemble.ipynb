{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25ea2bc4526998aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T23:16:25.288102Z",
     "start_time": "2025-11-17T23:15:56.577810Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    make_scorer,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:  # pragma: no cover - optional dependency\n",
    "    XGBOOST_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52696e2848e44df9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T23:30:14.800068200Z",
     "start_time": "2025-11-17T23:29:58.208800Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "DATA_FILE = PROJECT_ROOT / \"data\" / \"train\" / \"train_domains.csv\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "SCALER_PATH = MODELS_DIR / \"scaler.joblib\"\n",
    "METRICS_PATH = MODELS_DIR / \"training_metrics.json\"\n",
    "\n",
    "\n",
    "\n",
    "SCORING = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"precision\": make_scorer(precision_score),\n",
    "    \"recall\": make_scorer(recall_score),\n",
    "    \"f1\": \"f1\",\n",
    "    \"roc_auc\": make_scorer(roc_auc_score, needs_threshold=True),\n",
    "}\n",
    "\n",
    "RF_PARAM_GRID = {\n",
    "    \"n_estimators\": [200, 400],\n",
    "    \"max_depth\": [15, 20, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"],\n",
    "}\n",
    "\n",
    "XGB_PARAM_GRID = {\n",
    "    \"n_estimators\": [200, 400],\n",
    "    \"max_depth\": [3, 5],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "    \"subsample\": [0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.8, 1.0],\n",
    "} if XGBOOST_AVAILABLE else {}\n",
    "\n",
    "FEATURE_LABELS = [\n",
    "    \"length\",\n",
    "    \"entropy\",\n",
    "    \"consonant_vowel_ratio\",\n",
    "    \"vowel_count\",\n",
    "    \"consonant_count\",\n",
    "    \"digit_count\",\n",
    "    \"special_char_count\",\n",
    "    \"unique_char_count\",\n",
    "    \"max_consecutive_consonants\",\n",
    "    \"max_consecutive_same_char\",\n",
    "    \"pad_1\",\n",
    "    \"pad_2\",\n",
    "    \"pad_3\",\n",
    "    \"pad_4\",\n",
    "    \"pad_5\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "273dad326aee8673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    y_proba: Optional[np.ndarray] = None\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Return standard binary classification metrics.\"\"\"\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"f1\": f1_score(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "    if y_proba is not None and y_proba.size:\n",
    "        try:\n",
    "            metrics[\"roc_auc\"] = roc_auc_score(y_true, y_proba)\n",
    "        except ValueError:\n",
    "            metrics[\"roc_auc\"] = float(\"nan\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def extract_features(domain: str) -> np.ndarray:\n",
    "    \"\"\"Extract 15 features from domain\"\"\"\n",
    "    domain_name = domain.split('.')[0]\n",
    "    vowels = set('aeiouAEIOU')\n",
    "\n",
    "    features = []\n",
    "\n",
    "    # 1. Length\n",
    "    features.append(len(domain_name))\n",
    "\n",
    "    # 2. Entropy\n",
    "    if domain_name:\n",
    "        freq = {}\n",
    "        for c in domain_name:\n",
    "            freq[c] = freq.get(c, 0) + 1\n",
    "        entropy = -sum((count/len(domain_name)) * math.log2(count/len(domain_name))\n",
    "                       for count in freq.values() if count > 0)\n",
    "        features.append(entropy)\n",
    "    else:\n",
    "        features.append(0)\n",
    "\n",
    "    # 3. Consonant/vowel ratio\n",
    "    consonants = sum(1 for c in domain_name if c.lower() in 'bcdfghjklmnpqrstvwxyz')\n",
    "    vowel_count = sum(1 for c in domain_name if c in vowels)\n",
    "    features.append(consonants / (vowel_count + 1))\n",
    "\n",
    "    # 4. Vowel count\n",
    "    features.append(vowel_count)\n",
    "\n",
    "    # 5. Consonant count\n",
    "    features.append(consonants)\n",
    "\n",
    "    # 6. Digit count\n",
    "    features.append(sum(1 for c in domain_name if c.isdigit()))\n",
    "\n",
    "    # 7. Special char count\n",
    "    features.append(sum(1 for c in domain_name if not c.isalnum()))\n",
    "\n",
    "    # 8. Unique char count\n",
    "    features.append(len(set(domain_name)))\n",
    "\n",
    "    # 9. Max consecutive consonants\n",
    "    max_cons = 0\n",
    "    current = 0\n",
    "    for c in domain_name:\n",
    "        if c.lower() in 'bcdfghjklmnpqrstvwxyz':\n",
    "            current += 1\n",
    "            max_cons = max(max_cons, current)\n",
    "        else:\n",
    "            current = 0\n",
    "    features.append(max_cons)\n",
    "\n",
    "    # 10. Max consecutive same char\n",
    "    max_same = 0\n",
    "    if domain_name:\n",
    "        current = 1\n",
    "        for i in range(1, len(domain_name)):\n",
    "            if domain_name[i] == domain_name[i-1]:\n",
    "                current += 1\n",
    "                max_same = max(max_same, current)\n",
    "            else:\n",
    "                current = 1\n",
    "    features.append(max_same)\n",
    "\n",
    "    # 11-15. Padding to 15 features\n",
    "    features.extend([0] * (15 - len(features)))\n",
    "\n",
    "    return np.array(features[:15])\n",
    "\n",
    "def load_dataset(\n",
    "    csv_file: Path | str = DATA_FILE,\n",
    "    val_split: float = 0.15,\n",
    "    test_split: float = 0.15,\n",
    "    save_scaler: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Load dataset, create train/val/test splits, scale features, persist scaler.\"\"\"\n",
    "\n",
    "    csv_path = Path(csv_file)\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f\"Training dataset not found: {csv_path}\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Loading data...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    class_counts = df['label'].value_counts()\n",
    "    for label, count in class_counts.items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        print(f\"  Class {label}: {count} ({pct:.2f}%)\")\n",
    "\n",
    "    # Extract features matrix\n",
    "    print(\"\\nExtracting handcrafted features...\")\n",
    "    feature_matrix = np.vstack([extract_features(d) for d in df['domain']])\n",
    "    labels = df['label'].to_numpy()\n",
    "\n",
    "    # Split into train / test first\n",
    "    if not 0 < test_split < 1:\n",
    "        raise ValueError(\"test_split must be between 0 and 1\")\n",
    "    if not 0 < val_split < 1:\n",
    "        raise ValueError(\"val_split must be between 0 and 1\")\n",
    "\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        feature_matrix,\n",
    "        labels,\n",
    "        test_size=test_split,\n",
    "        random_state=42,\n",
    "        stratify=labels\n",
    "    )\n",
    "\n",
    "    # Derive validation ratio from remaining data\n",
    "    remaining = 1 - test_split\n",
    "    val_ratio = val_split / remaining\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp,\n",
    "        y_temp,\n",
    "        test_size=val_ratio,\n",
    "        random_state=42,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "\n",
    "    print(f\"Split sizes → train: {len(X_train)}, val: {len(X_val)}, test: {len(X_test)}\")\n",
    "\n",
    "    # Feature scaling (train only)\n",
    "    print(\"\\nFitting StandardScaler on train set...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    if save_scaler:\n",
    "        SCALER_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "        joblib.dump(scaler, SCALER_PATH)\n",
    "        print(f\"✓ Scaler saved to {SCALER_PATH}\")\n",
    "\n",
    "    dataset = {\n",
    "        'X_train': X_train_scaled,\n",
    "        'X_val': X_val_scaled,\n",
    "        'X_test': X_test_scaled,\n",
    "        'y_train': y_train,\n",
    "        'y_val': y_val,\n",
    "        'y_test': y_test,\n",
    "        'X_train_rnn': X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1)),\n",
    "        'X_val_rnn': X_val_scaled.reshape((X_val_scaled.shape[0], X_val_scaled.shape[1], 1)),\n",
    "        'X_test_rnn': X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1)),\n",
    "        'scaler': scaler,\n",
    "        'feature_names': FEATURE_LABELS,\n",
    "    }\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def train_lstm(data: dict):\n",
    "    \"\"\"Train LSTM model\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Training LSTM Model\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        layers.LSTM(64, return_sequences=True, input_shape=(15, 1)),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.LSTM(32),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    print(\"Model architecture:\")\n",
    "    model.summary()\n",
    "\n",
    "    print(\"\\nTraining...\")\n",
    "    history = model.fit(\n",
    "        data['X_train_rnn'], data['y_train'],\n",
    "        validation_data=(data['X_test_rnn'], data['y_test']),\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred_lstm = (model.predict(data['X_test_rnn'], verbose=0) > 0.5).astype(int).flatten()\n",
    "    precision = precision_score(data['y_test'], y_pred_lstm)\n",
    "    recall = recall_score(data['y_test'], y_pred_lstm)\n",
    "    f1 = f1_score(data['y_test'], y_pred_lstm)\n",
    "\n",
    "    print(f\"\\nLSTM Results:\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "\n",
    "    # Save\n",
    "    model_path = MODELS_DIR / 'lstm' / 'lstm_model.h5'\n",
    "    model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    model.save(model_path)\n",
    "    print(f\"✓ Model saved: {model_path}\")\n",
    "\n",
    "    return {'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "def train_gru(data: dict):\n",
    "    \"\"\"Train GRU model\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Training GRU Model\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        layers.GRU(64, return_sequences=True, input_shape=(15, 1)),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.GRU(32),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    print(\"Model architecture:\")\n",
    "    model.summary()\n",
    "\n",
    "    print(\"\\nTraining...\")\n",
    "    history = model.fit(\n",
    "        data['X_train_rnn'], data['y_train'],\n",
    "        validation_data=(data['X_test_rnn'], data['y_test']),\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred_gru = (model.predict(data['X_test_rnn'], verbose=0) > 0.5).astype(int).flatten()\n",
    "    precision = precision_score(data['y_test'], y_pred_gru)\n",
    "    recall = recall_score(data['y_test'], y_pred_gru)\n",
    "    f1 = f1_score(data['y_test'], y_pred_gru)\n",
    "\n",
    "    print(f\"\\nGRU Results:\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "\n",
    "    # Save\n",
    "    model_path = MODELS_DIR / 'gru' / 'gru_model.h5'\n",
    "    model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    model.save(model_path)\n",
    "    print(f\"✓ Model saved: {model_path}\")\n",
    "\n",
    "    return {'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "def train_rf(data: dict):\n",
    "    \"\"\"Train Random Forest model\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Training Random Forest Model\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=20,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"Training...\")\n",
    "    model.fit(data['X_train'], data['y_train'])\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred_rf = model.predict(data['X_test'])\n",
    "    precision = precision_score(data['y_test'], y_pred_rf)\n",
    "    recall = recall_score(data['y_test'], y_pred_rf)\n",
    "    f1 = f1_score(data['y_test'], y_pred_rf)\n",
    "\n",
    "    print(f\"\\nRandom Forest Results:\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "\n",
    "    # Feature importance\n",
    "    print(f\"\\nTop 5 Important Features:\")\n",
    "    importances = model.feature_importances_\n",
    "    for idx in np.argsort(importances)[-5:][::-1]:\n",
    "        print(f\"  Feature {idx}: {importances[idx]:.4f}\")\n",
    "\n",
    "    # Save\n",
    "    model_path = MODELS_DIR / 'rf' / 'rf_model.pkl'\n",
    "    model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"✓ Model saved: {model_path}\")\n",
    "\n",
    "    return {'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "\n",
    "def train_models(dataset: dict):\n",
    "    \"\"\"Train all models and persist metrics\"\"\"\n",
    "    lstm_results = train_lstm(dataset)\n",
    "    gru_results = train_gru(dataset)\n",
    "    rf_results = train_rf(dataset)\n",
    "\n",
    "    metrics = {\n",
    "        'lstm': lstm_results,\n",
    "        'gru': gru_results,\n",
    "        'rf': rf_results,\n",
    "        'ensemble_weights': {\n",
    "            'lstm': 0.33,\n",
    "            'gru': 0.33,\n",
    "            'rf': 0.34\n",
    "        }\n",
    "    }\n",
    "\n",
    "    METRICS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    METRICS_PATH.write_text(json.dumps(metrics, indent=2))\n",
    "    print(f\"\\n✓ Metrics saved: {METRICS_PATH}\")\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a450f060316cf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✗ Error during training: Training dataset not found: /content/C:\\Users\\rachk\\PycharmProjects\\dns_shield/data/train/train_domains.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipython-input-2448510258.py\", line 3, in <cell line: 0>\n",
      "    data = load_dataset(DATA_FILE)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipython-input-408101996.py\", line 103, in load_dataset\n",
      "    raise FileNotFoundError(f\"Training dataset not found: {csv_path}\")\n",
      "FileNotFoundError: Training dataset not found: /content/C:\\Users\\rachk\\PycharmProjects\\dns_shield/data/train/train_domains.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        data = load_dataset(DATA_FILE)\n",
    "        metrics = train_models(data)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"TRAINING SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        for name, values in metrics.items():\n",
    "            if isinstance(values, dict) and 'f1' in values:\n",
    "                print(f\"\\n{name.upper()} F1-Score: {values['f1']:.4f}\")\n",
    "\n",
    "        print(\"\\n✓ Training complete!\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error during training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c55a0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490ef60f1215952c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c646f997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4331172454da84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1707f21fdbaf6d41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fdb66a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e7cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de7700e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
